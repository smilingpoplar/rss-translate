<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>top scoring links : LocalLLaMA</title>
  <id>https://www.reddit.com/r/LocalLLaMA/top</id>
  <updated></updated>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <link href="https://www.reddit.com/r/LocalLLaMA/top"></link>
  <entry>
    <title>Meta在拥抱脸上发布了Mobilellm-R1</title>
    <updated>2025-09-12T16:35:23Z</updated>
    <id>t3_1nf7zhq</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/&#34;&gt;&lt;img src=&#34;https://preview.redd.it/huchm6bahrof1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f091dea3c1b3bd8cc946d3ae61d24b3e9a2e3a3b&#34; alt=&#34;Meta released MobileLLM-R1 on Hugging Face&#34; title=&#34;Meta在拥抱脸上发布了Mobilellm-R1&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;型号： &lt;a href=&#34;https://huggingface.co/facebook/MobileLLM-R1-950M&#34;&gt;https&lt;/a&gt; ：//huggingface.co/facebook/mobilellm-r1-950m&lt;/p&gt;&lt;p&gt; App（Vibe编码）： &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/MobileLLM-R1-950M&#34;&gt;https：//huggingface.co/spaces/akhaliq/mobilellm-r1-950m&lt;/a&gt;&lt;/p&gt;&lt;p&gt;应用程序在： &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/anycoder&#34;&gt;https：//huggingface.co/spaces/akhaliq/anycoder&lt;/a&gt;中&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/Illustrious_Row_9971&#34;&gt;/u /u /striplious_row_9971&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://i.redd.it/huchm6bahrof1.png&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/" rel="alternate"></link>
  </entry>
  <entry>
    <title>对于Qwen团队，请为QWEN3-NEXTS GGUF支持做出贡献！</title>
    <updated>2025-09-13T01:29:35Z</updated>
    <id>t3_1nfktdg</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/&#34;&gt;&lt;img src=&#34;https://b.thumbs.redditmedia.com/FW-gGORXDRiCTDczp9dCooqVX1C79rwrf1Z5V7TuZEM.jpg&#34; alt=&#34;To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!&#34; title=&#34;对于Qwen团队，请为QWEN3-NEXTS GGUF支持做出贡献！&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;如果您尚未注意到，Llama.cpp尚未支持Qwen3-Next，那是因为它带有自定义SSM Archiection。没有QWEN团队的支持，这种惊人的模型可能在数周甚至几个月内都无法支持。到目前为止，我坚信Llama.CPP第一天的支持是绝对必须的。&lt;/p&gt;&lt;p&gt; &lt;a href=&#34;https://preview.redd.it/uxcnskn54uof1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d39e54d6738531f99913794f3f2917609ff47b3&#34;&gt;https://preview.redd.it/uxcnskn54uof1.png?width=938＆format = png＆auto = webp&amp;amp;s = 5d39e54d67385385531f99999913794f3794f3794f3f3f29176099917609f47b3&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/Iory1998&#34;&gt;/u /iory1998&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/" rel="alternate"></link>
  </entry>
  <entry>
    <title>苹果偶然发现了MLX的成功</title>
    <updated>2025-09-12T17:50:47Z</updated>
    <id>t3_1nf9x9m</id>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;Qwen3-Next 80B-A3B在MLX上在拥抱脸上出现，MLX已经支持它。开源贡献者在24小时内完成了此操作。做苹果本身永远做不到的事情，仅仅是因为支持或不支持特定的中国人工智能公司，母公司可能会或不受特定的美国制裁，如果将苹果品牌在苹果品牌附近的任何地方都有几个月的时间，如果苹果在尝试并失败的过程中不让MLX在其研究手臂中演变为“ Apple Intelligence”，并且将其纳入公司，并将其吸引到中央，他们将其吸引到中央。这确实是一个故事的弧线，我认为他们的新M5芯片设计具有矩阵内核（更快的及时处理），他们实际上倾向于它！苹果从来都不是“独自一人”的选择，但现在实际上是…&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/Alarming-Ad8154&#34;&gt;/u /Alarming-AD8154&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/" rel="alternate"></link>
  </entry>
  <entry>
    <title>Qwen3 Next和DeepSeek v3.1在其推理和非争议模式下都具有相同的人工分析智能指数得分。</title>
    <updated>2025-09-12T15:48:49Z</updated>
    <id>t3_1nf6s0w</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/&#34;&gt;&lt;img src=&#34;https://preview.redd.it/hb62e80c7rof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45fbd3055204b4282742bcaf6567d07ade494ed5&#34; alt=&#34;Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes.&#34; title=&#34;Qwen3 Next和DeepSeek v3.1在其推理和非争议模式下都具有相同的人工分析智能指数得分。&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;由&lt;a href=&#34;https://www.reddit.com/user/R46H4V&#34;&gt;/u /r46h4v&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://i.redd.it/hb62e80c7rof1.png&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/" rel="alternate"></link>
  </entry>
  <entry>
    <title>我使用LLMS（GPT-oss：20b）制作了游戏 - 在LLMS中：您是冒名顶替者</title>
    <updated>2025-09-13T08:11:45Z</updated>
    <id>t3_1nfrzbv</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfrzbv/i_made_a_game_using_llms_gptoss20b_among_llms_you/&#34;&gt;&lt;img src=&#34;https://preview.redd.it/56ap88aa4wof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2062ede075a0f3745d9fa4bbb37dd8f7ed5d633&#34; alt=&#34;I made a game using LLMs (gpt-oss:20b) -- Among LLMs: You are the Impostor&#34; title=&#34;我使用LLMS（GPT-oss：20b）制作了游戏 - 在LLMS中：您是冒名顶替者&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;我使用Ollama和gpt-oss：OpenAi的20B模型在Python中制作了以下应用程序/游戏 - 对于喜欢看到和造成混乱的人，我喜欢像我这样的人。如果有兴趣，请检查一下。 github链接在最后。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;在LLM中，&lt;/strong&gt;您的&lt;strong&gt;终端&lt;/strong&gt;变成了一个混乱的聊天室游乐场，您是一群古怪的AI代理中唯一的人，掉入了一个共同的情况 - 可能是幻想，科幻，惊悚片，犯罪或完全出乎意料的东西。每个参与者（包括您）都有角色和背景故事，所有AI代理人都有一个共同的目标 - 通过投票来确定和消除人类。&lt;strong&gt;&lt;em&gt;您的任务：&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;保持隐藏，操纵对话，并用编辑，耳语，模仿和巧妙的气光互相将机器人互相交战&lt;/strong&gt;。大家都超越了所有人，使自己的优势变成混乱，并进入最后两个。&lt;/p&gt;&lt;p&gt;您可以在AI狩猎和超越AI的狩猎和&lt;em&gt;超越&lt;/em&gt;AI吗？&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;快速演示： &lt;a href=&#34;https://youtu.be/kbNe9WUQe14&#34;&gt;https：//youtu.be/kbne9wuqe14&lt;/a&gt;&lt;/p&gt;&lt;p&gt; github： &lt;a href=&#34;https://github.com/0xd3ba/among-llms&#34;&gt;https：//github.com/0xd3ba/among-llms&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/Alone-Foundation-134&#34;&gt;/u /单独创建134&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://i.redd.it/56ap88aa4wof1.png&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfrzbv/i_made_a_game_using_llms_gptoss20b_among_llms_you/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfrzbv/i_made_a_game_using_llms_gptoss20b_among_llms_you/" rel="alternate"></link>
  </entry>
  <entry>
    <title>RING-MINI-2.0 16B 1.4B MOE</title>
    <updated>2025-09-12T22:46:15Z</updated>
    <id>t3_1nfhbzv</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/&#34;&gt;&lt;img src=&#34;https://external-preview.redd.it/k_1ZiAjClo_PWHpZM0iAYeW3wPAsQ_ZQE2cc_xW7-3o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e8a42ffff0966a52b0cd60044e86cd61473738b&#34; alt=&#34;Ring-mini-2.0 16B 1.4b MoE&#34; title=&#34;RING-MINI-2.0 16B 1.4B MOE&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;由&lt;a href=&#34;https://www.reddit.com/user/HilLiedTroopsDied&#34;&gt;/u /u /hilliedtropopsdied&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://huggingface.co/inclusionAI/Ring-mini-2.0&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/" rel="alternate"></link>
  </entry>
  <entry>
    <title>VLLM现在支持Qwen3-Next：具有极高效率的混合体系结构</title>
    <updated>2025-09-12T23:33:53Z</updated>
    <id>t3_1nfieif</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/&#34;&gt;&lt;img src=&#34;https://external-preview.redd.it/K3rGlpkjbDPCdSyb_xOk55T-rqiVrUIviv6vZoP3TV0.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16235ad425456d8b12ef7e3e92930529dc005885&#34; alt=&#34;vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency&#34; title=&#34;VLLM现在支持Qwen3-Next：具有极高效率的混合体系结构&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;让我们起火吧！&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/chisleu&#34;&gt;/u /chisleu&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://blog.vllm.ai/2025/09/11/qwen3-next.html&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/" rel="alternate"></link>
  </entry>
  <entry>
    <title>QWEN3-NEXT-80B-A3B思维的长上下文测试。与QWEN3-30B-A3B-INCKINGING-2507相似，远远落后于QWEN3-235B-A22B-INCKING INKING</title>
    <updated>2025-09-12T15:00:15Z</updated>
    <id>t3_1nf5j8f</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf5j8f/long_context_tested_for_qwen3next80ba3bthinking/&#34;&gt;&lt;img src=&#34;https://preview.redd.it/9df1cyk90rof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b32809725e64ae3278a2a9f48134b0ee4513eb4e&#34; alt=&#34;Long context tested for Qwen3-next-80b-a3b-thinking. Performs very similarly to qwen3-30b-a3b-thinking-2507 and far behind qwen3-235b-a22b-thinking&#34; title=&#34;QWEN3-NEXT-80B-A3B思维的长上下文测试。与QWEN3-30B-A3B-INCKINGING-2507相似，远远落后于QWEN3-235B-A22B-INCKING INKING&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;由&lt;a href=&#34;https://www.reddit.com/user/fictionlive&#34;&gt;/u /fictionLive&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://i.redd.it/9df1cyk90rof1.png&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf5j8f/long_context_tested_for_qwen3next80ba3bthinking/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf5j8f/long_context_tested_for_qwen3next80ba3bthinking/" rel="alternate"></link>
  </entry>
  <entry>
    <title>Qwen3-next-80B-A3B：有关GGUF的新闻？</title>
    <updated>2025-09-12T15:30:42Z</updated>
    <id>t3_1nf6bgp</id>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;我一直在寻找HF，但似乎没有一个可用的东西，这似乎很奇怪。通常，随着高调发布，您会在一天之内看到一些。&lt;/p&gt;&lt;p&gt;那么，该模型现在是否有一些问题？有人在做它吗？&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/Herr_Drosselmeyer&#34;&gt;/u /herr_drosselmeyer&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf6bgp/qwen3next80ba3b_any_news_on_gguf/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf6bgp/qwen3next80ba3b_any_news_on_gguf/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf6bgp/qwen3next80ba3b_any_news_on_gguf/" rel="alternate"></link>
  </entry>
  <entry>
    <title>GPT-oss：20b＆qwen 4b是天堂制作的24GB VRAM构建的比赛</title>
    <updated>2025-09-12T18:05:08Z</updated>
    <id>t3_1nfaaik</id>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;我只是想分享，在尝试了几种型号（最近的QWEN-30B-A3B）之后，我发现GPT-oss：20b和Qwen4b一起加载到VRAM中，可以完美地平衡智能和速度，并为大约30K KV缓存提供了空间。我在需要推理的大多数与工作相关的查询中使用GPT-oss，QWEN 4B生成Web搜索查询。我也有QWEN4运行的PELPELIXA，它运行非常快 - （GPT -oss相当缓慢的返回结果）。&lt;/p&gt;&lt;p&gt;显然，YMMV，但想分享此设置，以防可能对他人有帮助。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/No_Information9314&#34;&gt;/u /no_information9314&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfaaik/gptoss20b_qwen_4b_are_a_match_made_in_heaven_for/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfaaik/gptoss20b_qwen_4b_are_a_match_made_in_heaven_for/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfaaik/gptoss20b_qwen_4b_are_a_match_made_in_heaven_for/" rel="alternate"></link>
  </entry>
  <entry>
    <title>对推理模型的痴迷是什么？</title>
    <updated>2025-09-13T06:33:35Z</updated>
    <id>t3_1nfqe2c</id>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;这只是一个迷你咆哮，所以我事先表示歉意。为什么在过去的几个月中，几乎所有AI模型都会发布所有推理模型？即使是那些不是现在的“混合思维”模型。就像每个AI Corpo都痴迷于当前的推理模型一样。&lt;/p&gt;&lt;p&gt;我个人不喜欢推理模型，感觉他们的唯一目的是以巨大的代币浪费为代价来回答棘手的谜语。&lt;/p&gt;&lt;p&gt;感觉一切都变得越来越台式。模型在难题和编码上过于努力，以创意写作和一般情报为代价。我认为一个很好的例子是DeepSeek v3.1，尽管从技术上讲，它比V3-0324更好地基准测试，但在许多方面都感觉像是一个更糟糕的模型。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/HadesThrowaway&#34;&gt;/u /hadesthrowaway&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/" rel="alternate"></link>
  </entry>
  <entry>
    <title>揭穿K2-思想的主张</title>
    <updated>2025-09-12T13:01:56Z</updated>
    <id>t3_1nf2mxj</id>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;在开放推理模型中，K2-think作为下一个时代被出售。但是，经过仔细检查，即使他们设法在测试数据上污染了它，但其性能并不比可比的竞争对手更好。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/nielstron&#34;&gt;/u /nielstron&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.sri.inf.ethz.ch/blog/k2think&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf2mxj/debunking_the_claims_of_k2think/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf2mxj/debunking_the_claims_of_k2think/" rel="alternate"></link>
  </entry>
  <entry>
    <title>qwen3 next（指令）编码基准结果</title>
    <updated>2025-09-12T16:52:33Z</updated>
    <id>t3_1nf8ff4</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf8ff4/qwen3_next_instruct_coding_benchmark_results/&#34;&gt;&lt;img src=&#34;https://external-preview.redd.it/XmbB_Ggpaw13Ih4SiltMb7pnW0SotFk3Ey3eZ2fkjxY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00fc58d8590fb833a39e70961d2fba37eab593c1&#34; alt=&#34;Qwen3 Next (Instruct) coding benchmark results&#34; title=&#34;qwen3 next（指令）编码基准结果&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;为什么我选择与您在链接上看到的替代方案进行比较：&lt;/p&gt;&lt;p&gt;就模型大小而言，“在本地运行是合理的”，这是将Qwen3与GPT-OSS-20B进行比较最有意义的。我还以“可能与OSS-20B相同的大小和托管供应商的价格相同”而扔进了GPT5-Nano，并且所有3个分数都相似。&lt;/p&gt;&lt;p&gt;但是，第三方推断供应商目前在3倍GPT-oss-20b上定价QWEN3，而阿里巴巴则将其差不多10倍（LOL）。因此，我还将GPT5-Mini和Flash 2.5包括在“阿里巴巴想要玩的同一价格类别中”，并且阿里巴巴在发行帖子（再次大声笑）中专门称呼“优于flash 2.5”。&lt;/p&gt;&lt;p&gt;因此：如果您在离散GPU上运行，请继续使用GPT-OSS-20B。如果您在Mac或新的Ryzen AI统一内存芯片上运行，则Qwen3 Next对于类似性能的速度应该更快。而且，如果您要推断推断，则可以以相同的价格获得相同的性能，或者以相同的价格获得更智能的型号。&lt;/p&gt;&lt;p&gt;注意：我试图仅针对阿里巴巴进行基准测试，但利率限制太低，所以我也添加了Deepinfra作为提供商。如果Deepinfra的内容错误配置错误，那么这些结果将被污染。我已经在链接上使用了Deepinfra的定价来获得成本效率图。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/mr_riptano&#34;&gt;/u /mr_riptano&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://brokk.ai/power-ranking?version=openround-2025-08-20&amp;amp;score=average&amp;amp;models=flash-2.5%2Cgpt-oss-20b%2Cgpt5-mini%2Cgpt5-nano%2Cq3next&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf8ff4/qwen3_next_instruct_coding_benchmark_results/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf8ff4/qwen3_next_instruct_coding_benchmark_results/" rel="alternate"></link>
  </entry>
  <entry>
    <title>Qwen3max感觉就像是必须参加灵敏度培训的经理</title>
    <updated>2025-09-13T01:54:45Z</updated>
    <id>t3_1nflbh4</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/&#34;&gt;&lt;img src=&#34;https://preview.redd.it/371xjrd89uof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1fc0fb0a562a6ddb6d774061259f8ebdfe2969e&#34; alt=&#34;Qwen3max feels like a manager that had to attend sensitivity training&#34; title=&#34;Qwen3max感觉就像是必须参加灵敏度培训的经理&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;我确实在现实生活中确实有这样的人。他绝对有点像频谱，根本没有幽默。人们告诉他要减轻，当他试图变得有趣时，情况变得更糟。&lt;/p&gt;&lt;p&gt;我的代码评论的其余部分不如第一行，但至少Qwen能够找到有关我的代码的一件好事。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/Coldaine&#34;&gt;/u /coldaine&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://i.redd.it/371xjrd89uof1.jpeg&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/" rel="alternate"></link>
  </entry>
  <entry>
    <title>Ollama用户的PSA：您的上下文长度可能低于您想象的</title>
    <updated>2025-09-12T21:35:08Z</updated>
    <id>t3_1nffm7r</id>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;我遇到了一个问题，发现Ollama默认为所有模型的4096上下文长度，无论该模型的实际功能如何。它默默地截断了任何其他上下文。我一直在检查官方的Ollama页面，并假设列出的上下文长度是默认情况下使用的。 Ollama PS命令​​，而不是Ollama Show &amp;lt;model-name&amp;gt;，最终揭示了所使用的真实上下文大小。如果您不是每天都在修补更改型号，请易于忽略。&lt;/p&gt;&lt;p&gt;您可以将其归结为用户无知，但是我想将其作为对初学者的警告：不要太兴奋地运行具有大的上下文窗口的模型，直到您明确设置它并检查了您的内存使用情况。我的主要反馈是Ollama网站更清楚地传达此默认设置。很高兴看到初学者参与运行本地设置，这只是他们的头脑。:)&lt;/p&gt;&lt;p&gt;对于许多当前的任务，4096上下文是非常有限的，尽管我理解为什么它可能是功能较小的硬件用户的默认值。它只需要更明确地进行传达。&lt;/p&gt;&lt;p&gt;更新：我承认我忽略了我的企业。当时我已经使用了Ollama很久了，我不确定这是否是。该帖子的目的只是新手的信息，因此他们更加了解。我曾认为，如果我没有明确设置在Env中，则会默认为模型的上下文。随意建议对新手用户友好的工具替代方案或指南。我们应该为他们促进一个热情的环境。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/gpt872323&#34;&gt;/u /gpt872323&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nffm7r/psa_for_ollama_users_your_context_length_might_be/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nffm7r/psa_for_ollama_users_your_context_length_might_be/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nffm7r/psa_for_ollama_users_your_context_length_might_be/" rel="alternate"></link>
  </entry>
  <entry>
    <title>Llama-OS-0.2.1-Beta +代码</title>
    <updated>2025-09-12T16:31:13Z</updated>
    <id>t3_1nf7vry</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf7vry/llamaos_021beta_code/&#34;&gt;&lt;img src=&#34;https://preview.redd.it/m8h48krffrof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a872d2a01db281e882ddef5b4721e6a01d375469&#34; alt=&#34;Llama-OS - 0.2.1-beta + Code&#34; title=&#34;Llama-os-0.2.1-beta +代码&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;大家好，&lt;/p&gt;&lt;p&gt;我已经发布了我的应用程序的代码&lt;br/&gt;&lt;a href=&#34;https://github.com/fredconex/Llama-OS&#34;&gt;https://github.com/fredconex/llama-os&lt;/a&gt;&lt;/p&gt;&lt;p&gt;对于有兴趣在行动中看到它的人，还有另一篇文章&lt;br/&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/&#34;&gt;https://www.reddit.com/r/localllama/comments/1nau0qe/llamaos_im_developing_an_app_to_to_to_make_make_llamacpp/&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/fredconex&#34;&gt;/u /fredconex&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://i.redd.it/m8h48krffrof1.png&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf7vry/llamaos_021beta_code/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf7vry/llamaos_021beta_code/" rel="alternate"></link>
  </entry>
  <entry>
    <title>Vaultgemma：世界上最有能力差异的私人LLM</title>
    <updated>2025-09-12T18:31:30Z</updated>
    <id>t3_1nfaye9</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfaye9/vaultgemma_the_worlds_most_capable_differentially/&#34;&gt;&lt;img src=&#34;https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d&#34; alt=&#34;VaultGemma: The world&#39;s most capable differentially private LLM&#34; title=&#34;Vaultgemma：世界上最有能力差异的私人LLM&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;由&lt;a href=&#34;https://www.reddit.com/user/vibjelo&#34;&gt;/u /vibjelo&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfaye9/vaultgemma_the_worlds_most_capable_differentially/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfaye9/vaultgemma_the_worlds_most_capable_differentially/" rel="alternate"></link>
  </entry>
  <entry>
    <title>我构建了当地的AI代理，该代理将我的杂乱计算机变成一个私人，可搜索的内存</title>
    <updated>2025-09-12T17:55:01Z</updated>
    <id>t3_1nfa11x</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfa11x/i_built_a_local_ai_agent_that_turns_my_messy/&#34;&gt;&lt;img src=&#34;https://external-preview.redd.it/_g7MxTDjiIWeWKTKuKZXglQRW6EdboZ0ViXwGjT4zqE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3a35d2cb24eeffc8ac9b7477450487d55ab5efc&#34; alt=&#34;I built a local AI agent that turns my messy computer into a private, searchable memory&#34; title=&#34;我构建了当地的AI代理，该代理将我的杂乱计算机变成一个私人，可搜索的内存&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;我自己的计算机是一团糟：黑曜石宣传，混乱的下载文件夹，随机会议笔记，无尽的PDF。我花了几个小时来挖掘一个我&lt;em&gt;知道&lt;/em&gt;在某个地方的信息 - 我敢肯定，仍然有很多宝贵的见解。&lt;/p&gt;&lt;p&gt;因此，我构建了&lt;strong&gt;超链接&lt;/strong&gt;- 设备AI代理，该代理搜索由本地AI模型提供动力的本地文件。 100％私人。离线工作。免费且无限。&lt;/p&gt;&lt;p&gt; &lt;a href=&#34;https://reddit.com/link/1nfa11x/video/fyfbgmuivrof1/player&#34;&gt;https://reddit.com/link/1nfa11x/video/fyfbgmuivrof1/player&lt;/a&gt;&lt;/p&gt;&lt;p&gt;我如何使用它：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;连接我的整个桌面，下载文件夹和黑曜石库（1000多个文件），并在几秒钟内扫描它们。我不再需要再次将更新的文件上传到聊天机器人！&lt;/li&gt;&lt;li&gt;像chatgpt一样询问您的PC，并在几秒钟内从文件中获取答案 - &amp;gt;在确切文件上进行内联引用。&lt;/li&gt;&lt;li&gt;针对特定文件夹（@Research_Notes），并将其“读取”仅像ChatGpt项目一样。因此，我可以在PC上组织我的“上下文”（文件）并直接与AI一起使用（不再重新上传/再次组织）&lt;/li&gt;&lt;li&gt; AI代理还了解图像（屏幕截图，扫描文档等）的文本&lt;/li&gt;&lt;li&gt;我还可以为不同的任务选择任何拥抱面部模型（GGUF + MLX）。我特别喜欢Openai的GPT-oss。感觉就像在我的PC上使用Chatgpt的大脑一样，但是无限的免费使用和完全隐私。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下载并尝试一下： &lt;a href=&#34;http://hyperlink.nexa.ai&#34;&gt;hyperlink.nexa.ai&lt;/a&gt;&lt;br/&gt;今天在Mac + Windows上工作，Arm Build即将推出。它是完全免费的私人使用的，我正在寻找扩展功能 - 旅行和反馈欢迎！也很想听到：您希望像这样的本地AI代理来解决什么样的用例？&lt;/p&gt;&lt;p&gt; HyperLink使用Nexa SDK（ &lt;a href=&#34;https://github.com/NexaAI/nexa-sdk&#34;&gt;https://github.com/nexaai/nexa-sdk&lt;/a&gt; ），这是一种开源的本地AI推理引擎。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/AlanzhuLy&#34;&gt;/u /alzhuly&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfa11x/i_built_a_local_ai_agent_that_turns_my_messy/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfa11x/i_built_a_local_ai_agent_that_turns_my_messy/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfa11x/i_built_a_local_ai_agent_that_turns_my_messy/" rel="alternate"></link>
  </entry>
  <entry>
    <title>Warllama：2x MI50 LLM MicroATX服务器</title>
    <updated>2025-09-13T06:55:54Z</updated>
    <id>t3_1nfqr69</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/&#34;&gt;&lt;img src=&#34;https://b.thumbs.redditmedia.com/h9qMX2YlXQIKQi4YG23oXTimROWCT0LUz66l7zJkwKg.jpg&#34; alt=&#34;WarLlama: 2x MI50 LLM MicroATX Server&#34; title=&#34;Warllama：2x MI50 LLM MicroATX服务器&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;该潜艇上的一些ppl使Ahab级的无畏型摇摆不定。其他有WAHORSE WA GIANT GPU或六个（或16倍？）。这是我时尚的Lil &lt;em&gt;Warllama&lt;/em&gt; 。&lt;/p&gt;&lt;p&gt;它不是在流传;这是奇妙的：我花多少钱建造一个小电力房屋。它出现了，但它本来是简约的 - 一个纯净的无头Linux盒，运行Llama.cpp + ROCM（需要从大量LLM使用中重新启动FREQ）WA Comfy 64GB VRAM。主要零件的成本： &lt;span class=&#34;md-spoiler-text&#34;&gt;730美元&lt;/span&gt;。如今，Bells＆Whistles的价格又花费了200美元以上，但我购买了最近的（超级）通货膨胀/关税BS。 YMMV。&lt;/p&gt;&lt;p&gt;警告：我在&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1j09qk1/dell_t640_for_a_4x_3090_build/&#34;&gt;Localllama Build指南&lt;/a&gt;中删除了每个明智的指南：超紧身案例，古老的桌面主板，怪异的GPU，越野车司机，甚至是笨拙的VBIOXEN，狭窄的气流。您可能会被刺激物食用。&lt;/p&gt;&lt;h2&gt;写部分：&lt;/h2&gt;&lt;ul&gt;&lt;li&gt; PC零件和成本&lt;/li&gt;&lt;li&gt;基准和温度&lt;/li&gt;&lt;li&gt;笔记&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;PC HW/SW零件和成本&lt;/h1&gt;&lt;h2&gt;HW&lt;/h2&gt;&lt;p&gt;都是模型，然后是GPU。主计算机是事后的想法。&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&#34;left&#34;&gt;价格&lt;/th&gt;&lt;th align=&#34;left&#34;&gt;部分&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt;$ 400&lt;/td&gt;&lt;td align=&#34;left&#34;&gt; 2x Mi50 32GB&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt; $ 130&lt;/td&gt;&lt;td align=&#34;left&#34;&gt;华硕Maximus VIII基因 + 32GB DDR4 + I5-6600K&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt; $ 35&lt;/td&gt;&lt;td align=&#34;left&#34;&gt;动力总成X100 PC情况&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt;$ 60&lt;/td&gt;&lt;td align=&#34;left&#34;&gt; Esgaming 750W模块化PSU&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt; $ 50&lt;/td&gt;&lt;td align=&#34;left&#34;&gt; 1TB NVME&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt; $ 17&lt;/td&gt;&lt;td align=&#34;left&#34;&gt; ARGB CPU粉丝&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt;$ 8&lt;/td&gt;&lt;td align=&#34;left&#34;&gt; 2倍三角洲球迷&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt;？&lt;/td&gt;&lt;td align=&#34;left&#34;&gt;各种3D打印机零件：风扇罩，I/O盾，GPU支架，PSU安装座&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt;$ 4&lt;/td&gt;&lt;td align=&#34;left&#34;&gt; 18pin色带电缆，用于扩展MI50周围的主板前面板销钉&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&#34;left&#34;&gt;&lt;strong&gt;总计：731美元&lt;/strong&gt;&lt;/td&gt;&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;钟声和哨子（不知道这些成本是多少）&lt;/h2&gt;&lt;ul&gt;&lt;li&gt; Razer Chroma ArgB控制器（6CH，完美的OpenRGB CTRL）&lt;/li&gt;&lt;li&gt; LCD 2004 + I2C ADAP&lt;/li&gt;&lt;li&gt; CH341：USB到I2C/GPIO&lt;/li&gt;&lt;li&gt; ARGB 120毫米盒风扇&lt;/li&gt;&lt;li&gt;内部USB开发的USB电缆/ADAP&lt;/li&gt;&lt;li&gt; 2X ARGB磁LED条&lt;/li&gt;&lt;li&gt;2x PCIE Y-Splitter用于GPU&lt;/li&gt;&lt;li&gt; VGA/HDMI CAR-REARVIEW显示器&lt;/li&gt;&lt;li&gt;ezoutlet5（穷人的BMC）&lt;/li&gt;&lt;li&gt;键盘&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;小于24pack苏打。像一只chonky猫一样沉重。&lt;/p&gt;&lt;ul&gt;&lt;li&gt; DIM：349 x 185 x 295mm（我认为19L）&lt;/li&gt;&lt;li&gt;总重量：19.3磅（8.68kg）&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt; SW&lt;/h2&gt;&lt;ul&gt;&lt;li&gt; Ubuntu 22.04 + 6.8 HWE内核&lt;/li&gt;&lt;li&gt;ROCM 6.4.1（6.4.4撕开Mi50 Supp！）&lt;/li&gt;&lt;li&gt; Llama.cpp-&amp;gt; build_rocm&lt;/li&gt;&lt;li&gt; VBIO：113-D1631700-111（发货W MI50的Orig Hacky Vbios）。&lt;/li&gt;&lt;li&gt; BIOS：V0402（MOBO首次有OEM BIOS BF更新）&lt;/li&gt;&lt;li&gt; OpenRGB（适用于Python argb ctrl）&lt;/li&gt;&lt;li&gt; &lt;a href=&#34;https://github.com/frank-zago/ch341-i2c-spi-gpio&#34;&gt;CH341 Linux驱动程序&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;基准和温度&lt;/h1&gt;&lt;p&gt;在下面发表评论&lt;/p&gt;&lt;h1&gt;笔记&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1m389gi/32gb_mi50_but_llamacpp_vulkan_sees_only_16gb/&#34;&gt;Mi50 Vbios不幸&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1j09qk1/dell_t640_for_a_4x_3090_build/&#34;&gt;建立Chonker多GPU钻机注意事项&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1mo92ou/rtx_5090_vs_rtx_4090_48gb_or_rtx_6000/&#34;&gt;我需要多少硬件？？？ VRAM食者与GPU卡特尔&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;您不能打扮垃圾，直到花钱花钱。这样的建筑SMTHG只能进行清晰的SW REQ评估和整个Lotta HW专业知识。 Old HW上的多GPU兼容是v Arcane； ESP W MI50。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;目标模型：QWEN家族。 V多功能，总部，指导。 v lil拒绝BS。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;用户酶：提交烹饪食谱，现代化Rolodex，对数十个表格细胞（！）进行算术。或ABT：ERP，潮湿的模因，导航计算（当我击中Lightspeed时，请不要穿星）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; MOBO是10 Yro，但是我拥有过的最光滑的董事会之一&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;它的奇迹我能够适应一切。 GPU，粉丝和坐骑。正常的ATX电缆长度。长（160毫米）全尺寸的ATX PSU。 SFF构建需要更多的零件才能使evryhting适合。定制的3D印刷塑料或能力（等功能电缆）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;同样，有足够的气流通过这样的SMOL空间，可以在Llama板凳上保持70c的距离&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我需要在主板的底部边缘扩展针头。 2.54mm的螺距带有电缆进行救援。仍然需要磨几个边，但它有效&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;我祈祷我的NVME将最后一次前进，BC ID需要将整个东西分开以交换驱动器。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;廉价的HW经济在业余爱好之外很糟糕。对于可行的企业，一个构建器需要每个盒子制造数千个。但是，没有人会以少于多GPU的庞然大物来付款。 DIY或死亡。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;由于此类绅士的软件进展，MI50似乎是P40的第二次降临。谢谢！ &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/15884&#34;&gt;Mi50的Flash Attn&lt;/a&gt; 。&lt;a href=&#34;https://github.com/iacopPBK/llama.cpp-gfx906&#34;&gt;部分2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt; 4X MI50钻机非常出色，但是Exps W 2X告诉我，对PCIE RSRC Alloc问题进行整理，对于多GPU而言，工作比平常更重要。而且仍然太过smol deepseek&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/__E8__&#34;&gt;/u /__ E8__&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/gallery/1nfqr69&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/" rel="alternate"></link>
  </entry>
  <entry>
    <title>释放包含/ling-mini-2.0</title>
    <updated>2025-09-13T00:30:10Z</updated>
    <id>t3_1nfjljo</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/&#34;&gt;&lt;img src=&#34;https://b.thumbs.redditmedia.com/oS8LL88rsyR-C5jtj0L74JwL83T5bIh5Z_l1ri9Sv7o.jpg&#34; alt=&#34;RELEASE inclusionAI/Ling-mini-2.0&#34; title=&#34;释放包含/ling-mini-2.0&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;伙计们，终于是只有CPU的模型，只需要量化！&lt;/p&gt;&lt;p&gt;包含AI四天前释放了Ling-Mini，现在是Ring（后者是一个思想实验）。&lt;/p&gt;&lt;p&gt;总参数为16B，但每个输入令牌只有1.4b被激活（非插入789m）。&lt;/p&gt;&lt;p&gt; &lt;a href=&#34;https://preview.redd.it/npuinnm4utof1.jpg?width=2418&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ee2e634e123fe35e911642324050e4ef1d471d5&#34;&gt;https://preview.redd.it/npuinnm4utof1.jpg?width=2418＆format = pjpg&amp;amp;auto = webppum = webp&amp;amp;s=8ee2e634e1234e123fe3E91166423242405050505050E4EF1D471D471D5&lt;/a&gt;&lt;/p&gt;&lt;p&gt;对于那些寻求无需GPU的功能解决方案的人来说，这是个好消息。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/juanlndd&#34;&gt;/u /juanlndd&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/" rel="alternate"></link>
  </entry>
  <entry>
    <title>GLM4.5空气与Qwen3-next-80b-a3b？</title>
    <updated>2025-09-12T21:00:41Z</updated>
    <id>t3_1nfeqhe</id>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;任何有Mac的人都有一些比较吗？&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/InsideYork&#34;&gt;/u /insional Yesional&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfeqhe/glm45_air_vs_qwen3next80ba3b/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfeqhe/glm45_air_vs_qwen3next80ba3b/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfeqhe/glm45_air_vs_qwen3next80ba3b/" rel="alternate"></link>
  </entry>
  <entry>
    <title>Olmo 3即将</title>
    <updated>2025-09-13T00:12:19Z</updated>
    <id>t3_1nfj81f</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfj81f/olmo_3_on_horizon/&#34;&gt;&lt;img src=&#34;https://external-preview.redd.it/u2yaRdnvlvy89ctXNWpzPO4FQ-KtPRHBd0SZB3Yz7X8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ece5a556d78a2b50b75460c7404069abc51078c&#34; alt=&#34;Olmo 3 on horizon&#34; title=&#34;Olmo 3即将&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;由&lt;a href=&#34;https://www.reddit.com/user/N8Karma&#34;&gt;/u /n8karma&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/pull/40778&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfj81f/olmo_3_on_horizon/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfj81f/olmo_3_on_horizon/" rel="alternate"></link>
  </entry>
  <entry>
    <title>Pytorch怀旧，有人吗？</title>
    <updated>2025-09-12T17:36:38Z</updated>
    <id>t3_1nf9k71</id>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;ML研究员和Pytorch的撰稿人。我真的很好奇：在过去的一年中，你们中有多少人从Pytorch的建筑转移到主要管理骆驼和其他型号的提示？与恒定的“提示 - &amp;gt; test-&amp;gt;重写”周期相比，您是否错过了旧的Pytorch工作流程 - 数据集，指标，培训循环？&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/dmpiergiacomo&#34;&gt;/u /dmpiergiacomo&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf9k71/pytorch_nostalgia_anyone/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf9k71/pytorch_nostalgia_anyone/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf9k71/pytorch_nostalgia_anyone/" rel="alternate"></link>
  </entry>
  <entry>
    <title>建立没有云的个人AI助手（2025指南）</title>
    <updated>2025-09-13T04:17:06Z</updated>
    <id>t3_1nfo1j4</id>
    <content type="html">&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfo1j4/building_a_personal_ai_assistant_without_the/&#34;&gt;&lt;img src=&#34;https://external-preview.redd.it/q0pbuZw5m9aebEJ2_iKV8VSARNL8LZSN6tvGWaFYcmY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac83b0c7db47906d8f0584938346fdc0b4901ecc&#34; alt=&#34;Building a Personal AI Assistant Without the Cloud (2025 Guide)&#34; title=&#34;建立没有云的个人AI助手（2025指南）&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;云助手很方便，但他们将您的数据发送到第三方服务器。在2025年，景观发生了变化：轻巧的开源LLM，有效的运行时间和离线语音堆栈使得可以完全在您的设备上运行有能力的AI助手。本指南将为您提供计划，工具，代码和部署，因此您可以建立一个隐私 - 脱机助手，以了解文本和声音，控制本地设备并完全在您的控制之下。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/nanhewa&#34;&gt;/u /nanhewa&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.lktechacademy.com/2025/09/building-personal-ai-assistant-without-cloud.html?m=1&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nfo1j4/building_a_personal_ai_assistant_without_the/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nfo1j4/building_a_personal_ai_assistant_without_the/" rel="alternate"></link>
  </entry>
  <entry>
    <title>帮助我Uderstand Moe模型。</title>
    <updated>2025-09-12T13:53:10Z</updated>
    <id>t3_1nf3ur7</id>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&#34;md&#34;&gt;&lt;p&gt;我的主要问题是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;为什么30B A3B模型比3B模型可以提供更好的结果？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果在某个时候使用所有30b的事实有任何不同，那么已知令牌的数量就不会减少吗？&lt;/p&gt;&lt;p&gt;纯粹是由于共享层吗？如果仍然只有3B参数，这有什么意义？&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;我目前的结论（非常感谢！）&lt;/p&gt;&lt;p&gt;每个令牌都是密集的模型结构上的涟漪，并且：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt; “为什么每当您已经知道浪潮最强的地方时，为什么都会模拟整个海洋波纹？”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;这来自一种理解，即密集模型中的一个令牌仅以有意义的方式影响网络的某些部分，因此，让我们专注于片段上它具有很小的精确损失。&lt;/p&gt;&lt;p&gt;就像顶部P采样器（或实际上可能是顶部K）一样，它只是切断了噪声而不会计算出来，因为它以最小的方式影响输出。&lt;/p&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;由&lt;a href=&#34;https://www.reddit.com/user/kaisurniwurer&#34;&gt;/u /kaisurniwurer&lt;/a&gt;提交&lt;br/&gt;&lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf3ur7/help_me_uderstand_moe_models/&#34;&gt;[链接]&lt;/a&gt;&lt;/span&gt; &lt;span&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/comments/1nf3ur7/help_me_uderstand_moe_models/&#34;&gt;[评论]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1nf3ur7/help_me_uderstand_moe_models/" rel="alternate"></link>
  </entry>
</feed>